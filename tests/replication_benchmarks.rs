// Generated by an AI assistant under the supervision of the project architect.

//! Replication Performance Benchmarks
//!
//! These tests demonstrate and measure replication performance under
//! various scenarios relevant to African infrastructure constraints.

use cityhall::{Entry, Wal};
use cityhall::replication::{ReplicationServer, ReplicationAgent};
use std::sync::Arc;
use parking_lot::RwLock;
use tempfile::tempdir;
use tokio::time::{sleep, Duration, Instant};

/// Benchmark: Offline Recovery - Measure catch-up speed
/// 
/// Simulates an 8-hour outage where replica is offline while leader
/// continues writing. Measures how fast replica catches up.
/// 
/// Scenario: Factory IoT gateway loses internet for 8 hours
#[tokio::test]
async fn bench_offline_recovery_catchup_speed() {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  BENCHMARK: Offline Recovery & Catch-Up Speed           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // Setup leader
    let leader_dir = tempdir().unwrap();
    let leader_path = leader_dir.path().join("leader.wal");
    
    let mut leader_wal = Wal::new(&leader_path, 1024).unwrap();
    leader_wal.set_segment_size_limit(50_000); // 50KB segments
    
    // Phase 1: Initial sync
    println!("ğŸ“ Phase 1: Writing initial data (100 entries)...");
    for i in 0..100 {
        let entry = Entry {
            key: format!("sensor.temp.line_{}", i).into_bytes(),
            value: format!("value_{}", i).into_bytes(),
            timestamp: 1000 + i,
        };
        leader_wal.append(&entry).unwrap();
    }
    leader_wal.flush().unwrap();
    
    let initial_segments = leader_wal.list_closed_segments().unwrap();
    println!("   Created {} initial segments", initial_segments.len());
    
    let leader_wal = Arc::new(RwLock::new(leader_wal));
    
    // Start replication server
    // Start replication server
    let server = ReplicationServer::new(Arc::clone(&leader_wal), 17001);
    tokio::spawn(async move {
        server.serve().await.unwrap();
    });
    sleep(Duration::from_millis(200)).await;
    
    // Setup replica and sync initial data
    let replica_dir = tempdir().unwrap();
    let replica_wal_path = replica_dir.path().join("replica.wal");
    let replica_state_path = replica_dir.path().join("replica_state.json");
    
    let replica_wal = Wal::new(&replica_wal_path, 1024).unwrap();
    let replica_wal = Arc::new(RwLock::new(replica_wal));
    
    let mut agent = ReplicationAgent::new(
        "127.0.0.1:17001".to_string(),
        "bench-replica".to_string(),
        replica_state_path.clone(),
        Arc::clone(&replica_wal),
    ).unwrap();
    
    println!("ğŸ”„ Phase 2: Initial sync to replica...");
    let sync_start = Instant::now();
    while agent.sync_once().await.unwrap() {
        // Keep syncing until caught up
    }
    let initial_sync_time = sync_start.elapsed();
    println!("   âœ“ Initial sync completed in {:?}", initial_sync_time);
    println!("   Replica at segment: {}", agent.state().last_synced_segment);
    
    // Phase 2: Simulate offline period - Leader continues writing
    println!("\nğŸ“ Phase 3: SIMULATING OFFLINE PERIOD");
    println!("   Replica offline while leader writes 500 more entries...");
    
    let offline_start = Instant::now();
    {
        let mut wal = leader_wal.write();
        for i in 100..600 {
            let entry = Entry {
                key: format!("sensor.temp.line_{}", i).into_bytes(),
                value: vec![0u8; 150], // ~150 bytes per entry
                timestamp: 1000 + i,
            };
            wal.append(&entry).unwrap();
        }
        wal.flush().unwrap();
    }
    
    let offline_segments = leader_wal.read().list_closed_segments().unwrap();
    let entries_missed = 500;
    println!("   âœ“ Leader wrote {} entries while replica offline", entries_missed);
    println!("   Leader now has {} closed segments", offline_segments.len());
    println!("   Offline period simulated: {:?}", offline_start.elapsed());
    
    // Phase 3: Replica comes back online - Measure catch-up
    println!("\nğŸ”„ Phase 4: REPLICA RECOVERY - Measuring catch-up speed...");
    
    let catchup_start = Instant::now();
    let mut segments_synced = 0;
    
    loop {
        match agent.sync_once().await {
            Ok(true) => {
                segments_synced += 1;
                let elapsed = catchup_start.elapsed();
                println!("   âœ“ Synced segment {} ({:?})", 
                         agent.state().last_synced_segment,
                         elapsed);
            }
            Ok(false) => {
                println!("   âœ“ Fully caught up!");
                break;
            }
            Err(e) => {
                panic!("Catch-up failed: {}", e);
            }
        }
    }
    
    let catchup_time = catchup_start.elapsed();
    let catchup_throughput = entries_missed as f64 / catchup_time.as_secs_f64();
    
    // Final verification
    let final_replica_segment = agent.state().last_synced_segment;
    let total_entries_applied = agent.state().total_entries_applied;
    
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                    BENCHMARK RESULTS                     â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Entries missed during offline: {:>6}                   â•‘", entries_missed);
    println!("â•‘  Segments to catch up:          {:>6}                   â•‘", segments_synced);
    println!("â•‘  Catch-up time:                 {:>6.2?}                â•‘", catchup_time);
    println!("â•‘  Catch-up throughput:           {:>6.0} entries/sec     â•‘", catchup_throughput);
    println!("â•‘  Final replica segment:         {:>6}                   â•‘", final_replica_segment);
    println!("â•‘  Total entries applied:         {:>6}                   â•‘", total_entries_applied);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // Assertions
    assert_eq!(total_entries_applied, 600, "Should have all 600 entries");
    assert!(catchup_time < Duration::from_secs(30), 
            "Catch-up should complete within 30 seconds");
    assert!(catchup_throughput > 10.0, 
            "Should sync at least 10 entries/sec");
    
    println!("âœ… BENCHMARK PASSED: Offline recovery within acceptable limits\n");
}

/// Benchmark: Multiple Replicas - Measure convergence time
/// 
/// Tests how quickly 3 replicas all converge to the same state
/// when syncing from the same leader.
#[tokio::test]
async fn bench_multiple_replicas_convergence() {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  BENCHMARK: Multiple Replicas Convergence               â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    const NUM_REPLICAS: usize = 3;
    const ENTRIES_COUNT: u64 = 200;
    
    // Setup leader
    let leader_dir = tempdir().unwrap();
    let leader_path = leader_dir.path().join("leader.wal");
    
    let mut leader_wal = Wal::new(&leader_path, 1024).unwrap();
    leader_wal.set_segment_size_limit(30_000);
    
    println!("ğŸ“ Writing {} entries to leader...", ENTRIES_COUNT);
    for i in 0..ENTRIES_COUNT {
        let entry = Entry {
            key: format!("metric.cpu.node_{}", i).into_bytes(),
            value: vec![0u8; 100],
            timestamp: 2000 + i,
        };
        leader_wal.append(&entry).unwrap();
    }
    leader_wal.flush().unwrap();
    
    let segments_count = leader_wal.list_closed_segments().unwrap().len();
    println!("   âœ“ Created {} segments", segments_count);
    
    let leader_wal = Arc::new(RwLock::new(leader_wal));
    
    // Start server
    let server = ReplicationServer::new(Arc::clone(&leader_wal), 17002);
    tokio::spawn(async move {
        server.serve().await.unwrap();
    });
    sleep(Duration::from_millis(200)).await;
    
    // Create multiple replicas and sync concurrently
    println!("\nğŸ”„ Starting {} replicas concurrently...", NUM_REPLICAS);
    
    let overall_start = Instant::now();
    let mut handles = vec![];
    
    for replica_id in 1..=NUM_REPLICAS {
        let replica_dir = tempdir().unwrap();
        let replica_wal_path = replica_dir.path().join("replica.wal");
        let replica_state_path = replica_dir.path().join("replica_state.json");
        
        let replica_wal = Wal::new(&replica_wal_path, 1024).unwrap();
        let replica_wal = Arc::new(RwLock::new(replica_wal));
        
        let mut agent = ReplicationAgent::new(
            "127.0.0.1:17002".to_string(),
            format!("replica-{}", replica_id),
            replica_state_path,
            replica_wal,
        ).unwrap();
        
        let handle = tokio::spawn(async move {
            let start = Instant::now();
            let mut segments_synced = 0;
            
            loop {
                match agent.sync_once().await {
                    Ok(true) => {
                        segments_synced += 1;
                    }
                    Ok(false) => {
                        break;
                    }
                    Err(e) => {
                        eprintln!("Replica {} error: {}", replica_id, e);
                        break;
                    }
                }
            }
            
            let sync_time = start.elapsed();
            let final_segment = agent.state().last_synced_segment;
            let total_entries = agent.state().total_entries_applied;
            
            println!("   âœ“ Replica {} synced {} segments in {:?}", 
                     replica_id, segments_synced, sync_time);
            
            (replica_id, sync_time, final_segment, total_entries)
        });
        
        handles.push(handle);
    }
    
    // Wait for all replicas
    let mut results = vec![];
    for handle in handles {
        results.push(handle.await.unwrap());
    }
    
    let overall_time = overall_start.elapsed();
    
    // Analyze results
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                    CONVERGENCE RESULTS                   â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    
    for (id, sync_time, segment, entries) in &results {
        println!("â•‘  Replica {}: {:>6.2?} | Seg: {:>3} | Entries: {:>4}      â•‘", 
                 id, sync_time, segment, entries);
    }
    
    let avg_time = results.iter()
        .map(|(_, t, _, _)| t.as_secs_f64())
        .sum::<f64>() / NUM_REPLICAS as f64;
    
    let max_time = results.iter()
        .map(|(_, t, _, _)| *t)
        .max()
        .unwrap();
    
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Overall convergence time:  {:>6.2?}                    â•‘", overall_time);
    println!("â•‘  Average sync time:         {:>6.2} seconds            â•‘", avg_time);
    println!("â•‘  Slowest replica:           {:>6.2?}                    â•‘", max_time);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // Verify all replicas have same state
    let first_segment = results[0].2;
    let first_entries = results[0].3;
    
    for (id, _, segment, entries) in &results {
        assert_eq!(*segment, first_segment, 
                   "Replica {} diverged: segment {} vs {}", 
                   id, segment, first_segment);
        assert_eq!(*entries, first_entries,
                   "Replica {} has different entry count: {} vs {}",
                   id, entries, first_entries);
    }
    
    assert!(overall_time < Duration::from_secs(10),
            "All replicas should converge within 10 seconds");
    
    println!("âœ… BENCHMARK PASSED: All replicas converged consistently\n");
}

/// Benchmark: Crash Recovery - State persistence across restarts
/// 
/// Verifies that replica state survives crashes and restarts without data loss.
/// Measures recovery time after crash.
#[tokio::test]
async fn bench_crash_recovery_with_state_persistence() {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  BENCHMARK: Crash Recovery & State Persistence          â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // Setup leader
    let leader_dir = tempdir().unwrap();
    let leader_path = leader_dir.path().join("leader.wal");
    
    let mut leader_wal = Wal::new(&leader_path, 1024).unwrap();
    leader_wal.set_segment_size_limit(20_000);
    
    println!("ğŸ“ Writing data to leader...");
    for i in 0..150 {
        let entry = Entry {
            key: format!("key_{}", i).into_bytes(),
            value: vec![0u8; 150],
            timestamp: 3000 + i,
        };
        leader_wal.append(&entry).unwrap();
    }
    leader_wal.flush().unwrap();
    
    let leader_wal = Arc::new(RwLock::new(leader_wal));
    
    let server = ReplicationServer::new(Arc::clone(&leader_wal), 17003);
    tokio::spawn(async move {
        server.serve().await.unwrap();
    });
    sleep(Duration::from_millis(200)).await;
    
    // Setup replica directory (PERSIST across "crashes")
    let replica_dir = tempdir().unwrap();
    let replica_wal_path = replica_dir.path().join("replica.wal");
    let replica_state_path = replica_dir.path().join("replica_state.json");
    
    // === RUN 1: Initial sync ===
    println!("\nğŸ”„ RUN 1: Initial sync before crash...");
    let run1_start = Instant::now();
    
    {
        let replica_wal = Wal::new(&replica_wal_path, 1024).unwrap();
        let replica_wal = Arc::new(RwLock::new(replica_wal));
        
        let mut agent = ReplicationAgent::new(
            "127.0.0.1:17003".to_string(),
            "crash-test-replica".to_string(),
            replica_state_path.clone(),
            replica_wal,
        ).unwrap();
        
        // Sync 2 segments, then "crash"
        agent.sync_once().await.unwrap();
        agent.sync_once().await.unwrap();
        
        let synced_segment = agent.state().last_synced_segment;
        let entries_applied = agent.state().total_entries_applied;
        
        println!("   âœ“ Synced {} segments ({} entries)", 
                 synced_segment, entries_applied);
        println!("   ğŸ’¥ SIMULATING CRASH (dropping agent)...");
        
        // Agent drops here, state should be persisted
    }
    
    let run1_time = run1_start.elapsed();
    println!("   Run 1 completed in {:?}", run1_time);
    
    // Verify state file exists
    assert!(replica_state_path.exists(), "State file should exist after crash");
    
    // Leader writes more data while replica is "crashed"
    println!("\nğŸ“ Leader writes more data while replica crashed...");
    {
        let mut wal = leader_wal.write();
        for i in 150..300 {
            let entry = Entry {
                key: format!("key_{}", i).into_bytes(),
                value: vec![0u8; 150],
                timestamp: 3000 + i,
            };
            wal.append(&entry).unwrap();
        }
        wal.flush().unwrap();
    }
    println!("   âœ“ Leader wrote 150 more entries");
    
    // === RUN 2: Recovery after crash ===
    println!("\nğŸ”„ RUN 2: Replica restarts and recovers...");
    let run2_start = Instant::now();
    
    {
        // Re-open WAL (simulates restart)
        let replica_wal = Wal::new(&replica_wal_path, 1024).unwrap();
        let replica_wal = Arc::new(RwLock::new(replica_wal));
        
        let mut agent = ReplicationAgent::new(
            "127.0.0.1:17003".to_string(),
            "crash-test-replica".to_string(), // Same ID
            replica_state_path.clone(),
            replica_wal,
        ).unwrap();
        
        let recovered_segment = agent.state().last_synced_segment;
        println!("   âœ“ Recovered state: last_synced_segment = {}", recovered_segment);
        assert_eq!(recovered_segment, 1, "Should recover state to segment 1");
        
        // Catch up from where we left off
        let mut segments_after_recovery = 0;
        loop {
            match agent.sync_once().await {
                Ok(true) => {
                    segments_after_recovery += 1;
                    println!("   âœ“ Caught up segment {}", 
                             agent.state().last_synced_segment);
                }
                Ok(false) => {
                    break;
                }
                Err(e) => {
                    panic!("Recovery failed: {}", e);
                }
            }
        }
        
        assert!(agent.state().last_synced_segment >= 2, "Should have synced up to at least segment 2 after recovery");
        let final_entries = agent.state().total_entries_applied;
        
        println!("   âœ“ Synced {} more segments after recovery", segments_after_recovery);
        println!("   âœ“ Final entry count: {}", final_entries);
        
        assert_eq!(final_entries, 300, "Should have all 300 entries after recovery");
    }
    
    let run2_time = run2_start.elapsed();
    
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                  CRASH RECOVERY RESULTS                  â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Run 1 (before crash):      {:>6.2?}                    â•‘", run1_time);
    println!("â•‘  Run 2 (after recovery):    {:>6.2?}                    â•‘", run2_time);
    println!("â•‘  State persisted:           âœ“ YES                       â•‘");
    println!("â•‘  Data loss:                 âœ“ NONE                      â•‘");
    println!("â•‘  Recovery time:             {:>6.2?}                    â•‘", run2_time);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    assert!(run2_time < Duration::from_secs(15),
            "Recovery should complete within 15 seconds");
    
    println!("âœ… BENCHMARK PASSED: State persisted, zero data loss\n");
}

/// Benchmark: Replication Latency - Measure time from write to replica
/// 
/// Measures the end-to-end latency: write to leader â†’ available on replica
#[tokio::test]
async fn bench_replication_latency() {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  BENCHMARK: Replication Latency (Write â†’ Replica)       â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // Setup leader
    let leader_dir = tempdir().unwrap();
    let leader_path = leader_dir.path().join("leader.wal");
    
    let mut leader_wal = Wal::new(&leader_path, 1024).unwrap();
    leader_wal.set_segment_size_limit(10_000); // Small segments for frequent rotation
    
    let leader_wal = Arc::new(RwLock::new(leader_wal));
    
    let server = ReplicationServer::new(Arc::clone(&leader_wal), 17004);
    tokio::spawn(async move {
        server.serve().await.unwrap();
    });
    sleep(Duration::from_millis(200)).await;
    
    // Setup replica
    let replica_dir = tempdir().unwrap();
    let replica_wal_path = replica_dir.path().join("replica.wal");
    let replica_state_path = replica_dir.path().join("replica_state.json");
    
    let replica_wal = Wal::new(&replica_wal_path, 1024).unwrap();
    let replica_wal = Arc::new(RwLock::new(replica_wal));
    
    let mut agent = ReplicationAgent::new(
        "127.0.0.1:17004".to_string(),
        "latency-replica".to_string(),
        replica_state_path,
        replica_wal,
    ).unwrap();
    
    // Measure latency for 10 sync cycles
    const CYCLES: usize = 5;
    let mut latencies = vec![];
    
    println!("ğŸ“Š Measuring replication latency over {} cycles...\n", CYCLES);
    
    for cycle in 1..=CYCLES {
        // Write 20 entries to leader
        let write_start = Instant::now();
        {
            let mut wal = leader_wal.write();
            for i in 0..20 {
                let entry = Entry {
                    key: format!("cycle_{}_entry_{}", cycle, i).into_bytes(),
                    value: vec![0u8; 100],
                    timestamp: 4000 + (cycle * 100) as u64 + i,
                };
                wal.append(&entry).unwrap();
            }
            wal.flush().unwrap();
        }
        let write_time = write_start.elapsed();
        
        // Wait for segment to close (force rotation)
        sleep(Duration::from_millis(50)).await;
        
        // Sync to replica and measure
        let sync_start = Instant::now();
        agent.sync_once().await.unwrap();
        let sync_time = sync_start.elapsed();
        
        let total_latency = write_start.elapsed();
        latencies.push(total_latency);
        
        println!("   Cycle {}: Write={:?}, Sync={:?}, Total={:?}", 
                 cycle, write_time, sync_time, total_latency);
    }
    
    // Calculate statistics
    let avg_latency = latencies.iter().sum::<Duration>() / CYCLES as u32;
    let min_latency = *latencies.iter().min().unwrap();
    let max_latency = *latencies.iter().max().unwrap();
    
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                    LATENCY RESULTS                       â•‘");
    println!("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
    println!("â•‘  Average latency:  {:>6.2?}                            â•‘", avg_latency);
    println!("â•‘  Min latency:      {:>6.2?}                            â•‘", min_latency);
    println!("â•‘  Max latency:      {:>6.2?}                            â•‘", max_latency);
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");
    
    // For LAN environments, should be under 1 second
    assert!(avg_latency < Duration::from_secs(2),
            "Average latency should be under 2 seconds");
    
    println!("âœ… BENCHMARK PASSED: Latency within acceptable range\n");
}
