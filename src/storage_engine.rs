use crate::compaction::{compact_sstables, select_sstables_for_compaction};
use crate::metrics::metrics;
use crate::sstable::{SsTableReader, SsTableWriter};
use crate::{Entry, MemTable, Result, Wal};
use crossbeam::channel::{self, Receiver, Sender};
use std::path::{PathBuf, Path};
use std::sync::atomic::{AtomicU64, Ordering};
use std::thread;
use std::time::{Duration, Instant, SystemTime};
use std::sync::Arc;
use parking_lot::RwLock;

const DEFAULT_BLOCK_SIZE: usize = 16 * 1024;

/// Message for background flush thread
enum FlushMessage {
    Flush {
        memtable: MemTable,
        path: PathBuf,
        sstable_id: u64,
    },
    Shutdown,
}

/// Result from background flush
struct FlushResult {
    #[allow(dead_code)]
    sstable_id: u64,
    path: PathBuf,
}

pub struct StorageEngine {
    wal: Arc<RwLock<Wal>>,
    memtable: MemTable,
    immutable_memtable: Option<MemTable>,
    sstables: Vec<SsTableReader>,
    #[allow(dead_code)]
    wal_path: PathBuf,
    data_dir: PathBuf,
    memtable_max_size: usize,
    sstable_counter: AtomicU64,

    flush_tx: Option<Sender<FlushMessage>>,
    flush_rx: Option<Receiver<FlushResult>>,
    _flush_thread: Option<thread::JoinHandle<()>>,
    background_flush_enabled: bool,

    compaction_enabled: bool,
    last_compaction_check: Instant,
}

#[derive(Debug, serde::Serialize)] // Added serde::Serialize
pub struct EngineStats {
    pub memtable_entries: usize,
    pub memtable_bytes: usize,
    pub num_sstables: usize,
    pub immutable_memtable_entries: usize,
}

impl StorageEngine {
    /// Create new StorageEngine with background flush enabled
    pub fn new(dir: PathBuf, memtable_max_size: usize, wal: Arc<RwLock<Wal>>) -> Result<Self> {
        Self::new_with_config(dir, memtable_max_size, wal, true)
    }

    /// Full constructor with configurable background flush
    pub fn new_with_config(
        dir: PathBuf,
        memtable_max_size: usize,
        wal: Arc<RwLock<Wal>>,
        background_flush: bool,
    ) -> Result<Self> {
        std::fs::create_dir_all(&dir)?;

        let wal_path = dir.join("data.wal");

        let entries = Wal::recover(&wal_path)?;
        let mut memtable = MemTable::new(memtable_max_size);
        for entry in entries {
            memtable.put(entry.key, entry.value, entry.timestamp)?;
        }

        // Load SSTables
        let mut sstables = Vec::new();
        let mut max_sstable_id = 0u64;
        for entry in std::fs::read_dir(&dir)? {
            let entry = entry?;
            let path = entry.path();
            if path.extension().and_then(|s| s.to_str()) == Some("sst") {
                if let Ok(reader) = SsTableReader::open(path.clone()) {
                    if let Some(stem) = path.file_stem() {
                        if let Some(id_str) = stem.to_str() {
                            if let Ok(id) = id_str.parse::<u64>() {
                                max_sstable_id = max_sstable_id.max(id);
                            }
                        }
                    }
                    sstables.push(reader);
                }
            }
        }
        sstables.sort_by_key(|r| {
            r.info()
                .path
                .file_stem()
                .and_then(|s| s.to_str())
                .and_then(|s| s.parse::<u64>().ok())
                .unwrap_or(0)
        });

        // Setup background flush
        let (flush_tx, flush_rx, flush_thread) = if background_flush {
            let (tx, rx_internal) = channel::unbounded();
            let (result_tx, rx) = channel::unbounded();
            let thread = Some(Self::spawn_flush_thread(rx_internal, result_tx));
            (Some(tx), Some(rx), thread)
        } else {
            (None, None, None)
        };

        Ok(StorageEngine {
            wal,
            memtable,
            immutable_memtable: None,
            sstables,
            wal_path,
            data_dir: dir,
            memtable_max_size,
            sstable_counter: AtomicU64::new(max_sstable_id + 1),
            flush_tx,
            flush_rx,
            _flush_thread: flush_thread,
            background_flush_enabled: background_flush,
            compaction_enabled: true,
            last_compaction_check: Instant::now(),
        })
    }

    /// Enable/disable compaction
    pub fn with_compaction(mut self, enabled: bool) -> Self {
        self.compaction_enabled = enabled;
        self
    }

    fn spawn_flush_thread(
        rx: Receiver<FlushMessage>,
        result_tx: Sender<FlushResult>,
    ) -> thread::JoinHandle<()> {
        thread::spawn(move || {
            while let Ok(msg) = rx.recv() {
                match msg {
                    FlushMessage::Flush {
                        memtable,
                        path,
                        sstable_id,
                    } => {
                        if let Err(e) = Self::flush_memtable_to_disk(memtable, &path) {
                            eprintln!("Background flush FAILED: {}", e);
                        } else {
                            let _ = result_tx.send(FlushResult { sstable_id, path });
                        }
                    }
                    FlushMessage::Shutdown => break,
                }
            }
        })
    }

    fn flush_memtable_to_disk(memtable: MemTable, path: &Path) -> Result<()> {
        let start = Instant::now();

        if memtable.is_empty() {
            return Ok(());
        }
        let mut writer = SsTableWriter::new(path.to_path_buf(), DEFAULT_BLOCK_SIZE)?;
        for (key, value, timestamp) in memtable.entries_with_timestamps() {
            writer.add(&key, &value, timestamp)?;
        }
        writer.finish()?;

        // Track flush
        metrics().flushes_total.inc();
        metrics().flush_duration.observe(start.elapsed());
        metrics().sstable_count.inc();

        // Note: disk usage updated by caller

        Ok(())
    }

    pub fn put(&mut self, key: Vec<u8>, value: Vec<u8>) -> Result<()> {
        let start = Instant::now();

        // Increment write counters
        metrics().writes_total.inc();
        metrics().writes_bytes.add((key.len() + value.len()) as u64);

        let timestamp = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)?
            .as_secs();
        let entry = Entry {
            key: key.clone(),
            value: value.clone(),
            timestamp,
        };

        // ‚úÖ Lock WAL, append, unlock
        {
            let mut wal_lock = self.wal.write();
            wal_lock.append(&entry)?;
            metrics().wal_size_bytes.set(wal_lock.size()?);
        } // Lock released here

        self.check_and_compact()?;

        let is_full = self.memtable.put(key, value, timestamp)?;
        if is_full {
            if self.background_flush_enabled {
                self.trigger_background_flush()?;
            } else {
                self.flush_memtable_sync()?;
            }
        }

        // Update memtable metrics
        metrics()
            .memtable_size_bytes
            .set(self.memtable.size_bytes() as u64);
        metrics().memtable_entries.set(self.memtable.len() as u64);

        // Record latency
        metrics().write_latency.observe(start.elapsed());

        Ok(())
    }

    fn trigger_background_flush(&mut self) -> Result<()> {
        let max_wait = 100;
        let mut waited = 0;
        while self.immutable_memtable.is_some() && waited < max_wait {
            self.check_flush_completion()?;
            if self.immutable_memtable.is_some() {
                thread::sleep(Duration::from_millis(1));
                waited += 1;
            }
        }
        if self.immutable_memtable.is_some() {
            return self.flush_memtable_sync();
        }

        let old_memtable =
            std::mem::replace(&mut self.memtable, MemTable::new(self.memtable_max_size));
        let sstable_id = self.sstable_counter.fetch_add(1, Ordering::SeqCst);
        let sstable_path = self.data_dir.join(format!("{:06}.sst", sstable_id));

        let entries_clone = old_memtable.entries_with_timestamps().clone();
        let mut immutable = MemTable::new(self.memtable_max_size);
        for (k, v, ts) in entries_clone {
            let _ = immutable.put(k, v, ts);
        }
        self.immutable_memtable = Some(immutable);

        if let Some(ref tx) = self.flush_tx {
            tx.send(FlushMessage::Flush {
                memtable: old_memtable,
                path: sstable_path,
                sstable_id,
            })?;
        }
        Ok(())
    }

    fn check_flush_completion(&mut self) -> Result<()> {
        // Collect results first to avoid holding reference to self.flush_rx
        let mut results = Vec::new();
        if let Some(ref rx) = self.flush_rx {
            while let Ok(result) = rx.try_recv() {
                results.push(result);
            }
        }

        // Process results without holding any borrows
        for result in results {
            let reader = SsTableReader::open(result.path)?;
            self.sstables.push(reader);
            self.immutable_memtable = None;

            // ‚úÖ NEW: Clean up WAL after successful flush
            self.cleanup_wal_after_flush()?;

            self.update_disk_usage();
        }

        Ok(())
    }

    fn flush_memtable_sync(&mut self) -> Result<()> {
        if self.memtable.is_empty() {
            return Ok(());
        }
        let sstable_id = self.sstable_counter.fetch_add(1, Ordering::SeqCst);
        let sstable_path = self.data_dir.join(format!("{:06}.sst", sstable_id));
        let memtable_to_flush =
            std::mem::replace(&mut self.memtable, MemTable::new(self.memtable_max_size));
        Self::flush_memtable_to_disk(memtable_to_flush, &sstable_path)?;
        self.sstables.push(SsTableReader::open(sstable_path)?);

        // ‚úÖ NEW: Clean up WAL after successful flush
        self.cleanup_wal_after_flush()?;

        self.update_disk_usage();
        Ok(())
    }

    /// ‚úÖ NEW: WAL cleanup helper method
    fn cleanup_wal_after_flush(&mut self) -> Result<()> {
        println!("\nüßπ Starting WAL cleanup after flush...");

        // ‚úÖ Lock WAL for all operations
        let mut wal_lock = self.wal.write();
        
        // Mark current segment as flushed
        wal_lock.mark_flushed()?;

        // Clean up old segments
        wal_lock.cleanup_old_segments(0)?;

        // Update WAL metrics
        if let Ok(wal_size) = wal_lock.size() {
            metrics().wal_size_bytes.set(wal_size);
            println!(
                "üìä Updated WAL size metric: {} bytes ({} MB)",
                wal_size,
                wal_size / 1_048_576
            );
        }
        
        drop(wal_lock); // Release lock

        Ok(())
    }

    pub fn get(&mut self, key: &[u8]) -> Result<Option<Vec<u8>>> {
        let start = Instant::now();

        // Track total read operation
        metrics().reads_total.inc();

        // Check active memtable
        if let Some(value) = self.memtable.get(key) {
            metrics().reads_hits.inc();
            metrics().read_latency.observe(start.elapsed());
            return Ok(Some(value));
        }

        // Check immutable memtable
        if let Some(immut) = &self.immutable_memtable {
            if let Some(value) = immut.get(key) {
                metrics().reads_hits.inc();
                metrics().read_latency.observe(start.elapsed());
                return Ok(Some(value));
            }
        }

        // Check SSTables (bloom filter check is inside sstable.get())
        for sstable in self.sstables.iter_mut().rev() {
            match sstable.get(key) {
                Ok(Some((value, _timestamp))) => {
                    metrics().reads_hits.inc();
                    metrics().read_latency.observe(start.elapsed());
                    return Ok(Some(value));
                }
                Ok(None) => {
                    // Bloom filter said "maybe" but key wasn't found
                    metrics().bloom_filter_false_positives.inc();
                    continue;
                }
                Err(crate::StorageError::CorruptedData(msg)) => {
                    eprintln!("Warning: corrupted SSTable, skipping: {}", msg);
                    continue;
                }
                Err(e) => return Err(e),
            }
        }

        // Key not found
        metrics().reads_misses.inc();
        metrics().read_latency.observe(start.elapsed());
        Ok(None)
    }

    pub fn scan(&mut self, start: &[u8], end: &[u8]) -> Result<Vec<(Vec<u8>, Vec<u8>, u64)>> {
        let mut results = self.memtable.scan_with_timestamps(start, end);
        if let Some(ref imm) = self.immutable_memtable {
            results.extend(imm.scan_with_timestamps(start, end));
        }
        for sstable in self.sstables.iter_mut() {
            results.extend(sstable.scan(start, end)?);
        }
        results.sort_by(|a, b| a.0.cmp(&b.0).then(b.2.cmp(&a.2)));
        results.dedup_by(|a, b| a.0 == b.0);
        Ok(results)
    }

    pub fn stats(&self) -> EngineStats {
        EngineStats {
            memtable_entries: self.memtable.len(),
            memtable_bytes: self.memtable.size_bytes(),
            num_sstables: self.sstables.len(),
            immutable_memtable_entries: self
                .immutable_memtable
                .as_ref()
                .map(|m| m.len())
                .unwrap_or(0),
        }
    }

    fn update_disk_usage(&mut self) {
        let mut total = 0u64;

        for sstable in &self.sstables {
            // Get file size from filesystem
            let info = sstable.info();
            if let Ok(metadata) = std::fs::metadata(&info.path) {
                total += metadata.len();
            }
        }

        // Add WAL size
        // ‚úÖ Lock WAL to read size
        {
            let wal_lock = self.wal.read();
            if let Ok(wal_size) = wal_lock.size() {
                total += wal_size;
                metrics().wal_size_bytes.set(wal_size);
            }
        } // Lock released

        metrics().disk_usage_bytes.set(total);
    }

    /// Compaction logic (updated)
    pub fn maybe_compact(&mut self) -> Result<()> {
        if !self.compaction_enabled {
            return Ok(());
        }

        let now = Instant::now();
        if now.duration_since(self.last_compaction_check) < Duration::from_secs(1) {
            return Ok(());
        }
        self.last_compaction_check = now;

        let sstable_paths: Vec<PathBuf> = std::fs::read_dir(&self.data_dir)?
            .filter_map(|e| e.ok())
            .map(|e| e.path())
            .filter(|p| p.extension().and_then(|s| s.to_str()) == Some("sst"))
            .collect();

        println!(
            "üîç Compaction check: {} SSTables found",
            sstable_paths.len()
        );

        if sstable_paths.len() < 4 {
            println!(
                "‚è≠Ô∏è  Skipping: need 4+ SSTables (have {})",
                sstable_paths.len()
            );
            return Ok(());
        }

        let to_compact = select_sstables_for_compaction(&sstable_paths, 4)?;

        if to_compact.is_empty() {
            println!("‚è≠Ô∏è  No suitable SSTables selected for compaction");
            return Ok(());
        }

        println!(
            "üîß Compaction needed: {} SSTables selected",
            to_compact.len()
        );

        self.compact_sstables_sync(to_compact)?;

        Ok(())
    }

    fn compact_sstables_sync(&mut self, input_paths: Vec<PathBuf>) -> Result<()> {
        println!("üóúÔ∏è  Starting compaction of {} SSTables", input_paths.len());

        let sstable_id = self.sstable_counter.fetch_add(1, Ordering::SeqCst);
        let output_path = self
            .data_dir
            .join(format!("{:06}_compacted.sst", sstable_id));

        let stats = compact_sstables(&input_paths, output_path.clone())?;
        let new_reader = SsTableReader::open(output_path)?;

        let before_count = self.sstables.len();
        self.sstables
            .retain(|reader| !input_paths.contains(&reader.info().path));
        let after_count = self.sstables.len();
        println!(
            "üìä Removed {} old SSTables from list",
            before_count - after_count
        );

        self.sstables.push(new_reader);
        println!("‚ûï Added compacted SSTable to list");

        for path in &input_paths {
            match std::fs::remove_file(path) {
                Ok(_) => println!("üóëÔ∏è  Deleted: {:?}", path.file_name()),
                Err(e) => eprintln!("‚ö†Ô∏è  Failed to delete {:?}: {}", path, e),
            }
        }

        println!(
            "‚úÖ Compaction complete: {} ‚Üí 1 SSTable, saved {}%",
            stats.input_sstables,
            ((stats.input_bytes - stats.output_bytes) * 100 / stats.input_bytes)
        );

        Ok(())
    }

    /// Force compaction (for testing)
    pub fn force_compact(&mut self) -> Result<()> {
        self.last_compaction_check = Instant::now() - Duration::from_secs(10);
        self.maybe_compact()
    }

    pub fn check_and_compact(&mut self) -> Result<()> {
        self.check_flush_completion()?;
        self.maybe_compact()?;
        Ok(())
    }

    pub fn sstable_count(&self) -> usize {
        self.sstables.len()
    }

    pub fn memtable_size(&self) -> usize {
        self.memtable.size_bytes()
    }

    /// Get formatted metrics summary
    pub fn metrics(&self) -> String {
        crate::metrics::metrics().summary()
    }

    /// Print metrics to stdout
    pub fn print_metrics(&mut self) {
        self.update_disk_usage();
        println!("{}", self.metrics());
    }

    /// Returns a shared reference to the WAL for use by replication
    pub fn get_wal(&self) -> Arc<RwLock<Wal>> {
        Arc::clone(&self.wal)
    }
}

impl Drop for StorageEngine {
    fn drop(&mut self) {
        if let Some(ref tx) = self.flush_tx {
            let _ = tx.send(FlushMessage::Shutdown);
        }
    }
}
